nn_cfg:
  lr: 1e-3 #1e-2
  method: adam
  intermediate_layer_sizes: [500, 500]
  batch_size: 100
  epochs: 1e6
  decay_lr: .1
  min_lr: 1e-7
  decay_every: 1e7

plateau_decay:
  min_lr: 1e-7
  decay_factor: 5
  avg_window_size: 5 # in epochs
  tolerance: 1e-3
  patience: 1


pretrain:
  pretrain_method: adam
  pretrain_stepsize: 1e-3
  pretrain_iters: 0
  pretrain_batches: 10

data:
  datetime: ''



eval_every_x_epochs: 10
save_every_x_epochs: 1
test_every_x_epochs: 1
write_csv_every_x_batches: 1
N_train: 10000  # adjust these for larger setups
N_test: 10
num_samples: 10
prediction_variable: w
angle_anchors: [0]

plot_iterates: [0, 10, 100]
loss_method: 'fixed_k' #'fixed_k' #'constant_sum'
share_all: False  # this is the shared solution approach
num_clusters: 1000
pretrain_alpha: False
normalize_inputs: True
normalize_alpha: 'other'
epochs_jit: 2
accuracies: [1, .1, .01, .001, .0001]

rho_x: 1
scale: 1
alpha_relax: 1
skip_startup: False  # can be used to skip the eval for no learn and NN 
solve_c_num: 10
save_weights_flag: True
eval_unrolls: 10000
train_unrolls: 1 # this is the k that tells you how many FOM iterations
supervised: False
load_weights_datetime: '2023-07-10/21-01-41'
# output_datetimes: ['2023-07-10/21-01-41', '2023-07-10/21-58-27', '2023-07-10/21-56-15', '2023-07-10/22-18-28', '2023-07-10/22-26-36', '2023-07-10/22-31-47', '2023-07-10/23-13-58', '2023-07-10/23-16-59', '2023-07-10/23-29-20', '2023-07-10/23-33-13']